# -*- coding: utf-8 -*-
"""step5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FZLveuHd_r-dYIZ4o8cmBmjwCgI_TwQV
"""



import pandas as pd
import numpy as np

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve

import shap
import matplotlib.pyplot as plt

DATA_PATH = "customer_pd_dataset.csv"
df = pd.read_csv(DATA_PATH)

if "default_flag_v3" not in df.columns:
    df["default_flag_v3"] = np.where(
        (df["return_rate"] >= 0.40) |
        ((df["return_rate"] >= 0.20) & (df["total_profit"] < 0) & (df["avg_discount"] >= 0.30)),
        1, 0
    )

TARGET = "default_flag_v3"

print("Bad rate:", df[TARGET].mean())
df.head()

leakage_features = [
    "return_rate",
    "avg_discount",
    "total_profit"
]

features = [
    c for c in df.columns
    if c != TARGET
    and pd.api.types.is_numeric_dtype(df[c])
    and c not in leakage_features
]

X = df[features].copy()
y = df[TARGET].astype(int).copy()

print("Features used:", features)

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.20,
    random_state=42,
    stratify=y
)

print("Train bad rate:", y_train.mean())
print("Test bad rate :", y_test.mean())

xgb_model = xgb.XGBClassifier(
    n_estimators=300,
    max_depth=3,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    objective="binary:logistic",
    eval_metric="auc",
    scale_pos_weight=(y_train.value_counts()[0] / y_train.value_counts()[1]),
    random_state=42
)

xgb_model.fit(X_train, y_train)

pd_test = xgb_model.predict_proba(X_test)[:, 1]

auc = roc_auc_score(y_test, pd_test)
prauc = average_precision_score(y_test, pd_test)

print(f"XGBoost ROC-AUC: {auc:.4f}")
print(f"XGBoost PR-AUC : {prauc:.4f}")

fpr, tpr, thresholds = roc_curve(y_test, pd_test)
ks = np.max(tpr - fpr)

print(f"XGBoost KS: {ks:.4f}")

plt.figure()
plt.plot(thresholds, tpr - fpr)
plt.title("KS Curve â€“ XGBoost")
plt.xlabel("Threshold")
plt.ylabel("KS")
plt.grid(True, alpha=0.3)
plt.show()

explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X_train)

shap.summary_plot(
    shap_values,
    X_train,
    plot_type="bar",
    show=True
)

shap.summary_plot(
    shap_values,
    X_train,
    show=True
)

top_features = features[:3]  # replace with actual top features from summary

for f in top_features:
    shap.dependence_plot(f, shap_values, X_train)

comparison = pd.DataFrame({
    "Model": ["Logistic Regression", "XGBoost"],
    "ROC-AUC": [0.6755, auc],
    "PR-AUC":  [0.3352, prauc]
})

comparison

