# -*- coding: utf-8 -*-
"""step2and3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_sZJ41EQ1bM8t4NbvyMO9lu4xUl31sVm
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

pd.set_option("display.max_columns", 200)
pd.set_option("display.float_format", lambda x: f"{x:,.4f}")

DATA_PATH = "customer_pd_dataset.csv"
df = pd.read_csv(DATA_PATH)

print("Shape:", df.shape)
df.head()

DATA_PATH = "customer_pd_dataset.csv"
df = pd.read_csv(DATA_PATH)

print("Shape:", df.shape)
df.head()

print(df.dtypes)

missing = df.isna().mean().sort_values(ascending=False)
display(missing[missing > 0].to_frame("missing_rate"))

#Finance expectation:
#not too tiny (<1%) and not too huge (>30%).
#If it is, we’ll adjust the proxy rule later.

assert "default_flag" in df.columns, "default_flag column not found"

bad_rate = df["default_flag"].mean()
print(f"Bad rate (default rate): {bad_rate:.2%}")

display(df["default_flag"].value_counts())
display(df["default_flag"].value_counts(normalize=True))

# good vs bad customers
numeric_cols = [c for c in df.columns if c != "default_flag"]

summary = (df
           .groupby("default_flag")[numeric_cols]
           .agg(["count","mean","median","min","max"])
          )
summary

# decide whether to:
#log-transform total_sales
#clip extreme values (winsorize)
#cap return_rate to [0,1]

df[numeric_cols].describe(percentiles=[.01,.05,.10,.25,.5,.75,.90,.95,.99]).T

def bad_rate_by_quantile(data, feature, q=10):
    d = data[[feature, "default_flag"]].dropna().copy()
    try:
        d["bin"] = pd.qcut(d[feature], q=q, duplicates="drop")
    except ValueError:
        d["bin"] = pd.qcut(d[feature], q=5, duplicates="drop")

    out = (d.groupby("bin")
             .agg(n=("default_flag","size"),
                  bad_rate=("default_flag","mean"),
                  feature_min=(feature,"min"),
                  feature_max=(feature,"max"))
             .reset_index(drop=True)
          )
    return out

features_to_bin = [c for c in numeric_cols if c in df.columns]

tables = {}
for f in features_to_bin:
    t = bad_rate_by_quantile(df, f, q=10)
    tables[f] = t
    print(f"\n=== Bad rate by {f} ===")
    display(t)

#We want drivers where risk increases/decreases consistently across bins.
def plot_bad_rate_curve(table, title):
    x = np.arange(len(table))
    y = table["bad_rate"].values

    plt.figure()
    plt.plot(x, y, marker="o")
    plt.title(title)
    plt.xlabel("Bin (low → high)")
    plt.ylabel("Bad rate")
    plt.grid(True, alpha=0.3)
    plt.show()

for f, t in tables.items():
    plot_bad_rate_curve(t, f"Bad rate by {f} (quantile bins)")

# This shows which features differ most between good and bad customers.
separation = []
for c in numeric_cols:
    good = df.loc[df["default_flag"] == 0, c].dropna()
    bad  = df.loc[df["default_flag"] == 1, c].dropna()
    if len(good) > 0 and len(bad) > 0:
        separation.append({
            "feature": c,
            "mean_good": good.mean(),
            "mean_bad": bad.mean(),
            "median_good": good.median(),
            "median_bad": bad.median(),
            "ratio_bad_to_good_mean": (bad.mean() / good.mean()) if good.mean() != 0 else np.nan
        })

sep_df = pd.DataFrame(separation).sort_values("ratio_bad_to_good_mean", ascending=False)
sep_df

corr = df[numeric_cols + ["default_flag"]].corr(numeric_only=True)
display(corr["default_flag"].sort_values(ascending=False))

plt.figure()
plt.imshow(corr.values)
plt.title("Correlation matrix (numeric)")
plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
plt.yticks(range(len(corr.columns)), corr.columns)
plt.colorbar()
plt.tight_layout()
plt.show()

# total_profit almost perfectly separates the target, consider:
#dropping it for modeling, OR
#transforming to something less “target-imprinted” (e.g., profit margin, avg profit/order),
#depending on your story.
if "total_profit" in df.columns:
    display(df.groupby("default_flag")["total_profit"].describe())

eda_decisions = []

eda_decisions.append({
    "Decision": "Bad rate",
    "Choice": f"{bad_rate:.2%}",
    "Reason": "Check class balance realism for PD proxy"
})

eda_decisions.append({
    "Decision": "Key drivers",
    "Choice": "Select features with monotonic bad-rate curves and clear separation",
    "Reason": "Preferred for stable scorecards and robust models"
})

if "total_profit" in df.columns:
    eda_decisions.append({
        "Decision": "Leakage caution",
        "Choice": "Review total_profit for target imprinting; may exclude from final model",
        "Reason": "Proxy default uses profit < 0"
    })

decision_df = pd.DataFrame(eda_decisions)
decision_df

#Default = 1 if ANY of the following is true:
#1) return_rate ≥ 30%
#OR
#2) (return_rate ≥ 15% AND total_profit < 0)
#OR
#3) avg_discount ≥ 40% AND total_profit < 0

customer_df = df.copy()  # or re-load your customer-level table

customer_df["default_flag_v2"] = np.where(
    (customer_df["return_rate"] >= 0.30) |
    ((customer_df["return_rate"] >= 0.15) & (customer_df["total_profit"] < 0)) |
    ((customer_df["avg_discount"] >= 0.40) & (customer_df["total_profit"] < 0)),
    1,
    0
)

# Check new bad rate
bad_rate_v2 = customer_df["default_flag_v2"].mean()
print(f"Updated bad rate: {bad_rate_v2:.2%}")

customer_df["default_flag_v2"].value_counts(normalize=True)

customer_df = df.copy()

customer_df["default_flag_v3"] = np.where(
    (customer_df["return_rate"] >= 0.40) |
    (
        (customer_df["return_rate"] >= 0.20) &
        (customer_df["total_profit"] < 0) &
        (customer_df["avg_discount"] >= 0.30)
    ),
    1,
    0
)

# Check new bad rate
bad_rate_v3 = customer_df["default_flag_v3"].mean()
print(f"Final bad rate (v3): {bad_rate_v3:.2%}")

customer_df["default_flag_v3"].value_counts(normalize=True)

#We want drivers where risk increases/decreases consistently across bins.
def plot_bad_rate_curve(table, title):
    x = np.arange(len(table))
    y = table["bad_rate"].values

    plt.figure()
    plt.plot(x, y, marker="o")
    plt.title(title)
    plt.xlabel("Bin (low → high)")
    plt.ylabel("Bad rate")
    plt.grid(True, alpha=0.3)
    plt.show()

for f, t in tables.items():
    plot_bad_rate_curve(t, f"Bad rate by {f} (quantile bins)")